{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data, preparing batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pre\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import copy\n",
    "import all_models as am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/bf-repo/preprocessing.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  apply(lambda x: x[0: 1000000])\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data('train_small.csv', 'val_small.csv', 'test_small.csv', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, input_size, embedding_size, hidden_size, output_size,\n",
    "                 num_layers, dropout, bidirectional, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.rnn = getattr(nn, rnn_type.upper())(embedding_size, hidden_size, num_layers, dropout=dropout,\n",
    "                                         bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        dropped_rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out\n",
    "    \n",
    "    def evaluate(self, preds, labels):\n",
    "        return self.loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.9057289958000183\n",
      "Epoch 1: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:0.911056637763977\n",
      "Epoch 2: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:0.9083573222160339\n",
      "Epoch 3: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8956953883171082\n",
      "Epoch 4: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8916605114936829\n",
      "Epoch 5: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.6293823719024658\n",
      "Epoch 6: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8864431381225586\n",
      "Epoch 7: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8886189460754395\n",
      "Epoch 8: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8981332778930664\n",
      "Epoch 9: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.897979199886322\n",
      "Epoch 10: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.1260080337524414\n",
      "Epoch 11: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8830670714378357\n",
      "Epoch 12: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8773015141487122\n",
      "Epoch 13: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8614024519920349\n",
      "Epoch 14: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8585597276687622\n",
      "Epoch 15: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8479180335998535\n",
      "Epoch 16: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8406187295913696\n",
      "Epoch 17: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.838347852230072\n",
      "Epoch 18: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8162755966186523\n",
      "Epoch 19: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:0.8559810519218445\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = False\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = RNN('lstm', INPUT_DIM, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE,\n",
    "         NUM_LAYERS, DROPOUT, BIDIRECTIONAL, PADDING_IDX)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 20\n",
    "tm = am.Training_module(model, LEARNING_RATE, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "\n",
    "#Training the model\n",
    "best_model_acc, best_model_prec, best_model_rec = tm.train_model(train_it, val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(10510, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 100, num_layers=2, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(10510, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 100, num_layers=2, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(10510, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 100, num_layers=2, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1269: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1269: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.757 | Test Acc: 68.89% | Test Prec: 0.00% | Test Rec: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1269: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "tm.model = best_model_acc\n",
    "test_loss, test_acc, test_prec, test_rec = tm.evaluate(test_it)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Test Prec: {test_prec*100:.2f}% | Test Rec: {test_rec*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = WordEmbAvg(INPUT_DIM, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, PADDING_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tm = am.Training_module(simple_nn, LEARNING_RATE, POS_WEIGHT, USE_CUDA, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:0.9329513907432556\n",
      "Epoch 1: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:0.9552356600761414\n",
      "Epoch 2: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.0157915353775024\n",
      "Epoch 3: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.0917619466781616\n",
      "Epoch 4: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.0736163854599\n",
      "Epoch 5: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.0844529867172241\n",
      "Epoch 6: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.2133562564849854\n",
      "Epoch 7: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.2626413106918335\n",
      "Epoch 8: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.1554826498031616\n",
      "Epoch 9: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.270661473274231\n",
      "Epoch 10: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.3522993326187134\n",
      "Epoch 11: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.442277431488037\n",
      "Epoch 12: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.2497369050979614\n",
      "Epoch 13: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.3181406259536743\n",
      "Epoch 14: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.4325379133224487\n",
      "Epoch 15: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:1.1891216039657593\n",
      "Epoch 16: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:1.3035739660263062\n",
      "Epoch 17: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:1.4680002927780151\n",
      "Epoch 18: Dev Accuracy: 0.6000000238418579 Dev Precision: 0.0 Dev Recall: 0.0 Dev Loss:1.1489629745483398\n",
      "Epoch 19: Dev Accuracy: 0.800000011920929 Dev Precision: nan Dev Recall: 0.0 Dev Loss:1.410115122795105\n"
     ]
    }
   ],
   "source": [
    "simple_best_model_acc, simple_best_model_prec, simple_best_model_rec = simple_tm.train_model(train_it, val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordEmbAvg(\n",
       "  (embedding): Embedding(10510, 100)\n",
       "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (drop_layer): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_best_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.694 | Test Acc: 55.56% | Test Prec: 16.67% | Test Rec: nan%\n"
     ]
    }
   ],
   "source": [
    "simple_tm.model = simple_best_model_acc\n",
    "simple_test_loss, simple_test_acc, simple_test_prec, simple_test_rec = simple_tm.evaluate(test_it)\n",
    "print(f'Test Loss: {simple_test_loss:.3f} | Test Acc: {simple_test_acc*100:.2f}% | Test Prec: {simple_test_prec*100:.2f}% | Test Rec: {simple_test_rec*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### magic loop? (for vanilla NN)\n",
    "\n",
    "* epochs\n",
    "* drop-outs\n",
    "* embedding\n",
    "* hidden\n",
    "* learning rate\n",
    "* class weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['learning_rate', 'pos_weight', 'num_epochs', 'drop_outs', 'embedding', 'hidden', 'accuracy', 'precision', 'recall', 'loss', 'model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_it' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1322e4f89258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPOS_WEIGHT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_pos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_it' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "\n",
    "for num_epoch in [5, 10, 20, 25]:\n",
    "    for dropout_rate in [0, 0.1, 0.25, 0.5, 0.75]:\n",
    "        for embed_dim in [20, 32, 64, 100]:\n",
    "            for hidden_dim in [10, 25, 40, 50]:\n",
    "                for learn_rate in [0.1, 0.01, 0.001]:\n",
    "                    mod = WordEmbAvg(INPUT_DIM, embed_dim, hidden_dim, OUTPUT_SIZE, PADDING_IDX, dropout_p=dropout_rate)\n",
    "                    trm = am.Training_module(mod, learn_rate, POS_WEIGHT, USE_CUDA, num_epoch)\n",
    "                    best_acc, best_prec, best_rec = trm.train_model(train_it, val_it)\n",
    "                    trm.model = best_acc\n",
    "                    test_loss, test_acc, test_prec, test_rec = trm.evaluate(test_it)\n",
    "                    row = [learn_rate, class_weight, num_epoch, dropout_rate, embed_dim, hidden_dim, test_acc*100, test_prec*100, test_rec*100, test_loss, \"most_accurate\"]\n",
    "                    df.loc[len(df)] = row\n",
    "                    trm.model = best_prec\n",
    "                    test_loss, test_acc, test_prec, test_rec = trm.evaluate(test_it)\n",
    "                    row = [learn_rate, class_weight, num_epoch, dropout_rate, embed_dim, hidden_dim, test_acc*100, test_prec*100, test_rec*100, test_loss, \"most_precise\"]\n",
    "                    df.loc[len(df)] = row\n",
    "                    trm.model = best_rec\n",
    "                    test_loss, test_acc, test_prec, test_rec = trm.evaluate(test_it)\n",
    "                    row = [learn_rate, class_weight, num_epoch, dropout_rate, embed_dim, hidden_dim, test_acc*100, test_prec*100, test_rec*100, test_loss, \"most_recall\"]\n",
    "                    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
