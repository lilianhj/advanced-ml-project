{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('projtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    100\n",
       "1.0     17\n",
       "Name: decision_binary, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['decision_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547008547008547"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "train, test, val, TEXT, LABEL = pp.get_data('projtrain.csv', 'projval.csv', 'projtest.csv', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print(TEXT.vocab.itos[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-22fb263e6629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LeGlove.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for leglove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Glove'"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/43567475/how-to-load-word-vector-model-binary-file-of-glove-cooccurence-bin\n",
    "# https://github.com/DocketScience/LeGloVe\n",
    "\n",
    "from glove import Glove\n",
    "leglove_model = Glove.load('LeGlove.model')\n",
    "\n",
    "# for leglove\n",
    "\n",
    "# TEXT.build_vocab(train_data, \n",
    "#                 max_size = MAX_VOCAB_SIZE, \n",
    "#                 vectors = \"glove.6B.100d\", \n",
    "#                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'petitioner', 'resident', 'cms', 'ex', 'facility', 'c.f.r', 'care', 'i.g']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32168"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.alj_text),\n",
    "    sort_within_batch = True, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        ##YOUR CODE HERE##\n",
    "        # Choose an optimizer. optim.Adam is a popular choice\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "          # batch.alj_text has the texts and batch.decision_binary has the labels.\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            ##YOUR CODE HERE##\n",
    "            \n",
    "            predictions = self.model(batch.alj_text).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.decision_binary)\n",
    "            accuracy = binary_accuracy(predictions, batch.decision_binary)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator, num_epochs=5):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        all_predicts = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "                \n",
    "                predictions = self.model(batch.alj_text).squeeze(1)\n",
    "                all_predicts.append(predictions)\n",
    "            \n",
    "                loss = self.loss_fn(predictions, batch.decision_binary)\n",
    "            \n",
    "                acc = binary_accuracy(predictions, batch.decision_binary)\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        #print(all_predicts)\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "#You can try many different embedding dimensions. Common values are 20, 32, 64, 100, 128, 512\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "#Get the index of the pad token using the stoi function\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32168"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3570x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  329,  ...,  262,  262,  262],\n",
      "        [  35, 4519,   62,  ...,   35,   35,   35],\n",
      "        [ 339, 3210,  417,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    3,  176,  ...,    1,    1,    1],\n",
      "        [  48, 1574,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2269x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  279,  ...,  262,  279,  279],\n",
      "        [9977, 7658, 6422,  ...,   35, 6015, 1737],\n",
      "        [9653,  102, 5167,  ...,  339, 1270, 1072],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 701x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  279,  262,  262],\n",
      "        [  35,   35, 8229,  ...,    4,   35,   35],\n",
      "        [ 339,  339,  811,  ...,  612,  339,  339],\n",
      "        ...,\n",
      "        [ 154,   48,   14,  ...,    1,    1,    1],\n",
      "        [  48,  176,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1384x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 329, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  62,  35,  ...,  35,  35,  35],\n",
      "        [339, 417, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154, 154, 154,  ...,   1,   1,   1],\n",
      "        [ 48,  48,  48,  ...,   1,   1,   1],\n",
      "        [176, 176, 176,  ...,   1,   1,   1]]) tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 12311x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  262,  279,  262],\n",
      "        [  35,   35,   35,  ...,   35, 4950,   35],\n",
      "        [ 339,  339,  339,  ...,  339,  780,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 31029x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  279,  279,  279],\n",
      "        [  35,   35,  235,  ...,  235, 9408, 6533],\n",
      "        [ 339,  339,  330,  ...,  330,  677, 6874],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 18777x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   279,   279,  ...,   279,   279,   262],\n",
      "        [13215,  1695,  8694,  ...,  4143,  4399,    35],\n",
      "        [   76,    76,  3462,  ...,    76,   102,   339],\n",
      "        ...,\n",
      "        [  215,     1,     1,  ...,     1,     1,     1],\n",
      "        [  121,     1,     1,  ...,     1,     1,     1],\n",
      "        [  244,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3095x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  262,  ...,  279,  262,  262],\n",
      "        [4579, 4406,   35,  ..., 4388,   35,   35],\n",
      "        [  76, 1459,  339,  ..., 5567,  339,  339],\n",
      "        ...,\n",
      "        [   2,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 179,    1,    1,  ...,    1,    1,    1],\n",
      "        [  93,    1,    1,  ...,    1,    1,    1]]) tensor([1., 1., 0., 0., 0., 0., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 491x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   262,  ...,   262,   262,   262],\n",
      "        [18256,    35,    35,  ...,    35,    35,    35],\n",
      "        [ 1508,   339,   339,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [ 6310,     1,     1,  ...,     1,     1,     1],\n",
      "        [   33,     1,     1,  ...,     1,     1,     1],\n",
      "        [   12,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 6676x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  262,  262,  ...,  279,  279,  279],\n",
      "        [1695,   35,   35,  ..., 1995, 1995, 3869],\n",
      "        [  76,  339,  339,  ..., 1459, 1459,  643],\n",
      "        ...,\n",
      "        [   3,    1,    1,  ...,    1,    1,    1],\n",
      "        [2688,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 876,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 569x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154, 154, 154,  ...,   1,   1,   1],\n",
      "        [ 48,  48,  48,  ...,   1,   1,   1],\n",
      "        [176, 176, 176,  ...,   1,   1,   1]]) tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1227x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  262,  262,  262],\n",
      "        [  35,   76,   35,  ...,   35,   35,   35],\n",
      "        [ 339, 2467,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,   24,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1564x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   262,  ...,  9752,   262,   279],\n",
      "        [   35,    35,    35,  ..., 15113,    35,  9798],\n",
      "        [  339,   339,   339,  ...,    35,   339,  9545],\n",
      "        ...,\n",
      "        [  154,    48,     1,  ...,     1,     1,     1],\n",
      "        [   48,   176,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1505x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   262,  ...,   262,   262,   279],\n",
      "        [   35,    35,    35,  ...,    35,    35,  6234],\n",
      "        [  339,   339,   339,  ...,   339,   339, 14414],\n",
      "        ...,\n",
      "        [ 1431,   176,     1,  ...,     1,     1,     1],\n",
      "        [   42,     1,     1,  ...,     1,     1,     1],\n",
      "        [24742,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 854x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  262,  262,  262],\n",
      "        [  35, 1882,   35,  ...,   35,   35,   35],\n",
      "        [ 339, 6216,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1613x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  262,  262],\n",
      "        [  35,   35,   35,  ..., 6599,   35,   35],\n",
      "        [ 339,  339,  339,  ..., 1508,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 668x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 279],\n",
      "        [ 35,  35,  35,  ...,  35,  35,   4],\n",
      "        [339, 339, 339,  ..., 339, 339, 612],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3289x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   279,  ...,   279,   279,   279],\n",
      "        [   35,    35,  4834,  ..., 26611,     4, 13366],\n",
      "        [  339,   339,   144,  ...,     8,   612,   596],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 921x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  279,  ...,  262,  262,  262],\n",
      "        [  35, 6546, 5331,  ...,   35,   35,   35],\n",
      "        [ 339, 6510, 5335,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2323x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  262,  262,  ...,  262,  262,  262],\n",
      "        [ 344,   35,   35,  ...,   35,   35,   35],\n",
      "        [5497,  339,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [  11,    1,    1,  ...,    1,    1,    1],\n",
      "        [   5,    1,    1,  ...,    1,    1,    1],\n",
      "        [5638,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1850x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  262,  ...,  262,  279,  279],\n",
      "        [2578,  846,   35,  ...,   35, 9204, 6553],\n",
      "        [1829,   92,  339,  ...,  339, 1037, 7509],\n",
      "        ...,\n",
      "        [ 368,    1,    1,  ...,    1,    1,    1],\n",
      "        [   4,    1,    1,  ...,    1,    1,    1],\n",
      "        [   5,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1420x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 539x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  262,  262],\n",
      "        [  35,   35,   35,  ..., 5190,   35,   35],\n",
      "        [ 339,  339,  339,  ...,  811,  339,  339],\n",
      "        ...,\n",
      "        [ 154,  154,   48,  ...,    1,    1,    1],\n",
      "        [  48,   48,  176,  ...,    1,    1,    1],\n",
      "        [ 176,  176,    1,  ...,    1,    1,    1]]) tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2487x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  262,  262,  ...,  262,  262,  262],\n",
      "        [2026,   35,   35,  ...,   35,   35,   35],\n",
      "        [4445,  339,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [  74,    1,    1,  ...,    1,    1,    1],\n",
      "        [  46,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 703,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2071x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   279,   262,  ...,   556,   279,   279],\n",
      "        [   35,  4810,    35,  ...,    54, 12123,  1355],\n",
      "        [  339,  1072,   339,  ...,    96, 31949, 10308],\n",
      "        ...,\n",
      "        [  154,    30,   176,  ...,     1,     1,     1],\n",
      "        [   48,    46,     1,  ...,     1,     1,     1],\n",
      "        [  176,   167,     1,  ...,     1,     1,     1]]) tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1078x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  262,  262,  279],\n",
      "        [  35,   35,   35,  ...,   35,   35, 9727],\n",
      "        [ 339,  339,  339,  ...,  339,  339, 2964],\n",
      "        ...,\n",
      "        [ 154,  176,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1342x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 820x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   279,  ...,   262,   262,   262],\n",
      "        [   35,    35,  4488,  ...,    35,    35,    35],\n",
      "        [  339,   339, 11020,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2658x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 6367x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  279,  279],\n",
      "        [  35,   35,   35,  ..., 4750, 2195, 2614],\n",
      "        [ 339,  339,  339,  ..., 2026,  811, 2866],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 6066x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  279,  262,  279],\n",
      "        [  35,   35, 1944,  ..., 8407,   35, 3990],\n",
      "        [ 339,  339, 3932,  ..., 1779,  339,    8],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3161x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  262,  ...,  279,  279,  262],\n",
      "        [5156, 9833,   35,  ..., 1995, 6784,   35],\n",
      "        [2718, 1508,  339,  ..., 2025, 4396,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 0., 1., 0., 0., 1., 1., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2432x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 424x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,  48,   1,  ...,   1,   1,   1],\n",
      "        [ 48, 176,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3376x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  262,  ...,  262,  262,  262],\n",
      "        [1084, 1737,   35,  ...,   35,   35,   35],\n",
      "        [   8,  758,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1783x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  262,  262],\n",
      "        [  35,   35,   35,  ..., 1408,   35,   35],\n",
      "        [ 339,  339,  339,  ..., 1674,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 0., 1., 1., 0., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1191x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   262,  ...,   262,   262,   262],\n",
      "        [11813,    35,    35,  ...,    35,    35,    35],\n",
      "        [    8,   339,   339,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [  154,    48,    48,  ...,     1,     1,     1],\n",
      "        [   48,   176,   176,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1469x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  262,  262,  262],\n",
      "        [  35,   35, 3024,  ...,   35,   35,   35],\n",
      "        [ 339,  339,   35,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,  338,    1,  ...,    1,    1,    1],\n",
      "        [  48,   54,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 10387x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  262,  279,  262],\n",
      "        [  35, 7137,   35,  ...,   35, 3476,   35],\n",
      "        [ 339,  147,  339,  ...,  339, 3663,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2364x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  207,   262,   262,  ...,   262,   262,   262],\n",
      "        [18246,    35,    35,  ...,    35,    35,    35],\n",
      "        [ 4108,   339,   339,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [  154,   176,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 4732x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 1., 1., 0., 0., 0., 1., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 5791x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   279,   279,  ...,   279,   279,   262],\n",
      "        [   35,  1881,  1937,  ...,  2606, 11769,    35],\n",
      "        [  339,  1270,  1527,  ...,  5418, 11485,   339],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1806x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  262,  262,  ...,  262,  262,  279],\n",
      "        [6553,   35,   35,  ...,   35,   35,   28],\n",
      "        [7509,  339,  339,  ...,  339,  339, 8602],\n",
      "        ...,\n",
      "        [3199,   48,    1,  ...,    1,    1,    1],\n",
      "        [ 107,  176,    1,  ...,    1,    1,    1],\n",
      "        [ 213,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 612x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[25562,   262,   262,  ...,   262,   262,   262],\n",
      "        [   35,    35,    35,  ...,    35,    35,    35],\n",
      "        [  339,   339,   339,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2968x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 5267x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   279,   279,  ...,   262,   262,   279],\n",
      "        [ 7413, 12573,  2591,  ...,    35,    35,  5496],\n",
      "        [ 2059,  5533,    35,  ...,   339,   339,  1018],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 4478x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   262,  ...,   262,   262,   262],\n",
      "        [11250,    35,    35,  ...,    35,    35,    35],\n",
      "        [ 7008,   339,   339,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [   35,     1,     1,  ...,     1,     1,     1],\n",
      "        [    8,     1,     1,  ...,     1,     1,     1],\n",
      "        [   37,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1676x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  262,  279,  279],\n",
      "        [  35,   35,   35,  ...,   35,  580, 1761],\n",
      "        [ 339,  339,  339,  ...,  339,  363, 3725],\n",
      "        ...,\n",
      "        [ 154,    4,    1,  ...,    1,    1,    1],\n",
      "        [  48,   65,    1,  ...,    1,    1,    1],\n",
      "        [ 176,   38,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 742x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  262,  262,  262],\n",
      "        [  35,   35, 2779,  ...,   35,   35,   35],\n",
      "        [ 339,  339, 6222,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,   48,  176,  ...,    1,    1,    1],\n",
      "        [  48,  176,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1919x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 1., 0., 1., 0., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3818x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  262,  262,  279],\n",
      "        [  35,   35,   35,  ...,   35,   35, 4396],\n",
      "        [ 339,  339,  339,  ...,  339,  339, 6720],\n",
      "        ...,\n",
      "        [ 154,  154,    1,  ...,    1,    1,    1],\n",
      "        [  48,   48,    1,  ...,    1,    1,    1],\n",
      "        [ 176,  176,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 5099x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  279,  262,  262],\n",
      "        [  35, 1445,   35,  ..., 5581,   35,   35],\n",
      "        [ 339,    8,  339,  ...,  224,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3917x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  262,  262,  262],\n",
      "        [  35, 2158,   35,  ...,   35,   35,   35],\n",
      "        [ 339, 1449,  339,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2546x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  262,  262,  ...,  279,  262,  262],\n",
      "        [7323,   35,   35,  ..., 6235,   35,   35],\n",
      "        [  76,  339,  339,  ..., 1449,  339,  339],\n",
      "        ...,\n",
      "        [  33,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 129,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 207,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3479x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   279,  ...,   262,   262,   279],\n",
      "        [   35,    35, 12746,  ...,    35,    35,    14],\n",
      "        [  339,   339,    76,  ...,   339,   339,    35],\n",
      "        ...,\n",
      "        [  154,    48,     1,  ...,     1,     1,     1],\n",
      "        [   48,   176,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1292x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 279,  279,  262,  ...,  279,  279,  262],\n",
      "        [  51, 5190,   35,  ..., 3707, 2847,   35],\n",
      "        [  92, 5649,  339,  ..., 6023,  346,  339],\n",
      "        ...,\n",
      "        [   7,    1,    1,  ...,    1,    1,    1],\n",
      "        [9342,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 744,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2210x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  279,  262,  262],\n",
      "        [  35,   35, 5990,  ..., 2663,   35,   35],\n",
      "        [ 339,  339,   76,  ..., 5551,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.alj_text]:[torch.LongTensor of size 78276x4]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 4]\n",
      "tensor([[ 9752,   262,   279,   279],\n",
      "        [15113,    35,  2221,   417],\n",
      "        [   35,   339,  3937,   298],\n",
      "        ...,\n",
      "        [    9,     1,     1,     1],\n",
      "        [   80,     1,     1,     1],\n",
      "        [  464,     1,     1,     1]]) tensor([1., 1., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 4225x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  262,  262],\n",
      "        [  35,   35,   35,  ..., 2542,   35,   35],\n",
      "        [ 339,  339,  339,  ..., 1882,  339,  339],\n",
      "        ...,\n",
      "        [ 154,   48,    1,  ...,    1,    1,    1],\n",
      "        [  48,  176,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 956x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   262,  ...,   279,   279,   279],\n",
      "        [17682,    35,    35,  ..., 10224, 10224,  5167],\n",
      "        [17631,   339,   339,  ...,    92,    92,  1072],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 0., 0., 1., 0., 1., 0., 1., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 9058x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   262,  ...,   262,   279,   262],\n",
      "        [   35,    35,    35,  ...,    35,  1959,    35],\n",
      "        [  339,   339,   339,  ...,   339, 27839,   339],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 0., 0., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 7249x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  279,  262,  279],\n",
      "        [  35, 7968,   35,  ..., 4828,   35,  780],\n",
      "        [ 339,  996,  339,  ...,    8,  339, 3347],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2756x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  279,  262,  ...,  262,  279,  262],\n",
      "        [  35, 1693,   35,  ...,   35, 8693,   35],\n",
      "        [ 339, 3401,  339,  ...,  339, 7725,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 4054x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   279,  ...,   262,   262,   262],\n",
      "        [10353,    35,  2221,  ...,    35,    35,    35],\n",
      "        [ 7736,   339,  3937,  ...,   339,   339,   339],\n",
      "        ...,\n",
      "        [  424,     1,     1,  ...,     1,     1,     1],\n",
      "        [   72,     1,     1,  ...,     1,     1,     1],\n",
      "        [  286,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 7993x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   279,   262,  ...,   279,   279,  9752],\n",
      "        [ 2356,  3932,    35,  ...,  4682,  3483, 15113],\n",
      "        [ 3148,  3711,   339,  ...,   591,   102,    35],\n",
      "        ...,\n",
      "        [ 2096,     1,     1,  ...,     1,     1,     1],\n",
      "        [  118,     1,     1,  ...,     1,     1,     1],\n",
      "        [    7,     1,     1,  ...,     1,     1,     1]]) tensor([1., 0., 0., 1., 0., 1., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2165x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  262,  262],\n",
      "        [  35,   35,   35,  ..., 2663,   35,   35],\n",
      "        [ 339,  339,  339,  ..., 5551,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1744x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  262,   262,   279,  ...,   262,   279,   279],\n",
      "        [   35,    35,  6649,  ...,    35, 17969, 19128],\n",
      "        [  339,   339,  6728,  ...,   339, 11611, 13187],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 3654x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  279,  ...,  262,  262,  262],\n",
      "        [  35,   35, 5496,  ...,   35,   35,   35],\n",
      "        [ 339,  339,    8,  ...,  339,  339,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([1., 0., 0., 1., 0., 0., 0., 1., 1., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2007x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[  279,   262,   279,  ...,   279,   262,   262],\n",
      "        [ 3534,    35,  3337,  ..., 11328,    35,    35],\n",
      "        [   35,   339,  4486,  ...,  2191,   339,   339],\n",
      "        ...,\n",
      "        [  154,     1,     1,  ...,     1,     1,     1],\n",
      "        [   48,     1,     1,  ...,     1,     1,     1],\n",
      "        [  176,     1,     1,  ...,     1,     1,     1]]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 2897x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[ 262,  262,  262,  ...,  279,  279,  262],\n",
      "        [  35,   35,   35,  ..., 3879,  846,   35],\n",
      "        [ 339,  339,  339,  ..., 1245,   76,  339],\n",
      "        ...,\n",
      "        [ 154,    1,    1,  ...,    1,    1,    1],\n",
      "        [  48,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 176,    1,    1,  ...,    1,    1,    1]]) tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1119x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154, 176,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 10]\n",
      "\t[.alj_text]:[torch.LongTensor of size 1020x10]\n",
      "\t[.decision_binary]:[torch.FloatTensor of size 10]\n",
      "tensor([[262, 262, 262,  ..., 262, 262, 262],\n",
      "        [ 35,  35,  35,  ...,  35,  35,  35],\n",
      "        [339, 339, 339,  ..., 339, 339, 339],\n",
      "        ...,\n",
      "        [154,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 48,   1,   1,  ...,   1,   1,   1],\n",
      "        [176,   1,   1,  ...,   1,   1,   1]]) tensor([1., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    print(batch.alj_text, batch.decision_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5260844975709915\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49399032195409137\n",
      "Epoch 2: Dev Accuracy: 0.8444444437821707 Dev Loss:0.48934957136710483\n",
      "Epoch 3: Dev Accuracy: 0.7944444417953491 Dev Loss:0.499497930208842\n",
      "Epoch 4: Dev Accuracy: 0.7444444497426351 Dev Loss:0.5466504593690237\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "tm = Training_module(model)\n",
    "\n",
    "#Training the model\n",
    "best_model = tm.train_model(train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.512 | Test Acc: 84.40%\n"
     ]
    }
   ],
   "source": [
    "tm.model = best_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4312, 10.8774,  8.9406,  ...,  9.8766,  8.7714,  9.7347])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = torch.norm(best_model.embedding.weight, p=2, dim=1, keepdim=True).data.squeeze()\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_10 = norms.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uppergastrointestinal\n",
      "fluorescent\n",
      "dwight\n",
      "426.425(c)(1\n",
      "405.874(c)(5\n",
      "p-12\n",
      "overrule\n",
      "fore\n",
      "require\n",
      "cr2171\n"
     ]
    }
   ],
   "source": [
    "for i in highest_10:\n",
    "    print(TEXT.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6384853621323904\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6164842446645101\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6005518039067587\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5879337390263876\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5775904506444931\n",
      "Test Loss: 0.646 | Test Acc: 84.40%\n"
     ]
    }
   ],
   "source": [
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model = model.to(device)\n",
    "tm = Training_module(model)\n",
    "tm.optimizer = optim.Adagrad(tm.model.parameters(), lr=1e-3)\n",
    "optim_model = tm.train_model(train_iterator, valid_iterator)\n",
    "tm.model = optim_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6937061349550883\n",
      "Epoch 1: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6936950087547302\n",
      "Epoch 2: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6937471727530161\n",
      "Epoch 3: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6939022243022919\n",
      "Epoch 4: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6940129399299622\n",
      "Test Loss: 0.693 | Test Acc: 81.90%\n"
     ]
    }
   ],
   "source": [
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX, two_layers=False)\n",
    "model = model.to(device)\n",
    "tm = Training_module(model)\n",
    "onelayer_model = tm.train_model(train_iterator, valid_iterator)\n",
    "tm.model = onelayer_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate: 0\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5628097454706827\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49027171234289807\n",
      "Epoch 2: Dev Accuracy: 0.8444444437821707 Dev Loss:0.49950480461120605\n",
      "Epoch 3: Dev Accuracy: 0.7944444417953491 Dev Loss:0.4926765412092209\n",
      "Epoch 4: Dev Accuracy: 0.7444444497426351 Dev Loss:0.5308692256609598\n",
      "Test Loss: 0.573 | Test Acc: 84.40%\n",
      "Dropout rate: 0.1\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5035866896311442\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5168760567903519\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.505170946319898\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4701434224843979\n",
      "Epoch 4: Dev Accuracy: 0.8444444437821707 Dev Loss:0.5218525975942612\n",
      "Test Loss: 0.537 | Test Acc: 84.40%\n",
      "Dropout rate: 0.25\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.531109074751536\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5939091295003891\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5641109347343445\n",
      "Epoch 3: Dev Accuracy: 0.8277777830759684 Dev Loss:0.5490983525911967\n",
      "Epoch 4: Dev Accuracy: 0.7944444417953491 Dev Loss:0.5595276107390722\n",
      "Test Loss: 0.563 | Test Acc: 84.40%\n",
      "Dropout rate: 0.5\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6277357886234919\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5025594234466553\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5966071089108785\n",
      "Epoch 3: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6749525119860967\n",
      "Epoch 4: Dev Accuracy: 0.7944444417953491 Dev Loss:0.5531854331493378\n",
      "Test Loss: 0.563 | Test Acc: 84.40%\n",
      "Dropout rate: 0.75\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6578697562217712\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6523579061031342\n",
      "Epoch 2: Dev Accuracy: 0.8240740746259689 Dev Loss:0.6774643063545227\n",
      "Epoch 3: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6515858272711436\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6295685370763143\n",
      "Test Loss: 0.655 | Test Acc: 84.40%\n"
     ]
    }
   ],
   "source": [
    "for dropout_rate in [0, 0.1, 0.25, 0.5, 0.75]:\n",
    "    print(f'Dropout rate: {dropout_rate}')\n",
    "    model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX, dropout_p=dropout_rate)\n",
    "    model = model.to(device)\n",
    "    tm = Training_module(model)\n",
    "    dropout_model = tm.train_model(train_iterator, valid_iterator)\n",
    "    tm.model = dropout_model\n",
    "    test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: 20, Hidden dimensions: 10\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5548453231652578\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.522187352180481\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4997718979914983\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4898165613412857\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4761844724416733\n",
      "Test Loss: 0.562 | Test Acc: 84.40%\n",
      "Embedding dimensions: 20, Hidden dimensions: 25\n",
      "Epoch 0: Dev Accuracy: 0.8444444437821707 Dev Loss:0.6399096151192983\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5413826902707418\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5082739442586899\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49478266139825183\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4860636691252391\n",
      "Test Loss: 0.554 | Test Acc: 84.40%\n",
      "Embedding dimensions: 20, Hidden dimensions: 40\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5809823075930277\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5122327109177908\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49764351546764374\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.487284392118454\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4875638832648595\n",
      "Test Loss: 0.591 | Test Acc: 84.40%\n",
      "Embedding dimensions: 20, Hidden dimensions: 50\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.582994262377421\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5200085043907166\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49911776681741077\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49713168044885\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49439266820748645\n",
      "Test Loss: 0.596 | Test Acc: 84.40%\n",
      "Embedding dimensions: 32, Hidden dimensions: 10\n",
      "Epoch 0: Dev Accuracy: 0.6944444477558136 Dev Loss:0.6793851653734843\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5671759148438772\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5037440707286199\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4808450440565745\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47122140725453693\n",
      "Test Loss: 0.578 | Test Acc: 84.40%\n",
      "Embedding dimensions: 32, Hidden dimensions: 25\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5805943111578623\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5174727588891983\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4850035309791565\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4713260581096013\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4734866718451182\n",
      "Test Loss: 0.590 | Test Acc: 84.40%\n",
      "Embedding dimensions: 32, Hidden dimensions: 40\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5632591297229131\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5009885827700297\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.48664263884226483\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4896204322576523\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4905497059226036\n",
      "Test Loss: 0.574 | Test Acc: 84.40%\n",
      "Embedding dimensions: 32, Hidden dimensions: 50\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5397479832172394\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.48577868441740674\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4800712863604228\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47376300891240436\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4647776534159978\n",
      "Test Loss: 0.553 | Test Acc: 84.40%\n",
      "Embedding dimensions: 64, Hidden dimensions: 10\n",
      "Epoch 0: Dev Accuracy: 0.42592593530813855 Dev Loss:0.6926775276660919\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5310685286919276\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4872557024161021\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.486679141720136\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.48715802033742267\n",
      "Test Loss: 0.537 | Test Acc: 84.40%\n",
      "Embedding dimensions: 64, Hidden dimensions: 25\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.6063348352909088\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5056129147609075\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4971123735109965\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.48774335781733197\n",
      "Epoch 4: Dev Accuracy: 0.8444444437821707 Dev Loss:0.48342712471882504\n",
      "Test Loss: 0.619 | Test Acc: 84.40%\n",
      "Embedding dimensions: 64, Hidden dimensions: 40\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5296905736128489\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47335146367549896\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47054026772578555\n",
      "Epoch 3: Dev Accuracy: 0.8444444437821707 Dev Loss:0.47574588159720105\n",
      "Epoch 4: Dev Accuracy: 0.7944444417953491 Dev Loss:0.48281338314215344\n",
      "Test Loss: 0.540 | Test Acc: 84.40%\n",
      "Embedding dimensions: 64, Hidden dimensions: 50\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5139835576216379\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47520794967810315\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4831349203983943\n",
      "Epoch 3: Dev Accuracy: 0.8444444437821707 Dev Loss:0.47842032214005786\n",
      "Epoch 4: Dev Accuracy: 0.8111111124356588 Dev Loss:0.49001382291316986\n",
      "Test Loss: 0.522 | Test Acc: 84.40%\n",
      "Embedding dimensions: 100, Hidden dimensions: 10\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5624340623617172\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.49658409754435223\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4881434539953868\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47788339853286743\n",
      "Epoch 4: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47585229575634\n",
      "Test Loss: 0.566 | Test Acc: 84.40%\n",
      "Embedding dimensions: 100, Hidden dimensions: 25\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5882250765959421\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5047335028648376\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.48972440759340924\n",
      "Epoch 3: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4948105563720067\n",
      "Epoch 4: Dev Accuracy: 0.8444444437821707 Dev Loss:0.4785122200846672\n",
      "Test Loss: 0.601 | Test Acc: 84.40%\n",
      "Embedding dimensions: 100, Hidden dimensions: 40\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.536737322807312\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5122436036666235\n",
      "Epoch 2: Dev Accuracy: 0.8444444437821707 Dev Loss:0.4929092774788539\n",
      "Epoch 3: Dev Accuracy: 0.7944444417953491 Dev Loss:0.509640172123909\n",
      "Epoch 4: Dev Accuracy: 0.7444444497426351 Dev Loss:0.511652834713459\n",
      "Test Loss: 0.529 | Test Acc: 84.40%\n",
      "Embedding dimensions: 100, Hidden dimensions: 50\n",
      "Epoch 0: Dev Accuracy: 0.8611111144224802 Dev Loss:0.5146411657333374\n",
      "Epoch 1: Dev Accuracy: 0.8611111144224802 Dev Loss:0.4772438059250514\n",
      "Epoch 2: Dev Accuracy: 0.8611111144224802 Dev Loss:0.47646328806877136\n",
      "Epoch 3: Dev Accuracy: 0.8111111124356588 Dev Loss:0.4695533563693364\n",
      "Epoch 4: Dev Accuracy: 0.827777773141861 Dev Loss:0.4765973836183548\n",
      "Test Loss: 0.515 | Test Acc: 84.40%\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['embedding', 'hidden', 'accuracy'])\n",
    "for embed_dimension in [20, 32, 64, 100]:\n",
    "    for hidden_dimension in [10, 25, 40, 50]:\n",
    "        print(f'Embedding dimensions: {embed_dimension}, Hidden dimensions: {hidden_dimension}')\n",
    "        model = WordEmbAvg(INPUT_DIM, embed_dimension, hidden_dimension, OUTPUT_DIM, PAD_IDX)\n",
    "        model = model.to(device)\n",
    "        tm = Training_module(model)\n",
    "        dim_model = tm.train_model(train_iterator, valid_iterator)\n",
    "        tm.model = dim_model\n",
    "        test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        row = [embed_dimension, hidden_dimension, test_loss*100]\n",
    "        #row = [embed_dimension, hidden_dimension, test_acc*100]\n",
    "        df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>hidden</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>56.247431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>55.409171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>59.124228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.612226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57.804537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>58.998898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>57.403537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.301298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.735669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>61.885237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>53.980426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>52.182407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>56.649998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>60.134274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>52.920101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.548753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    embedding  hidden   accuracy\n",
       "0        20.0    10.0  56.247431\n",
       "1        20.0    25.0  55.409171\n",
       "2        20.0    40.0  59.124228\n",
       "3        20.0    50.0  59.612226\n",
       "4        32.0    10.0  57.804537\n",
       "5        32.0    25.0  58.998898\n",
       "6        32.0    40.0  57.403537\n",
       "7        32.0    50.0  55.301298\n",
       "8        64.0    10.0  53.735669\n",
       "9        64.0    25.0  61.885237\n",
       "10       64.0    40.0  53.980426\n",
       "11       64.0    50.0  52.182407\n",
       "12      100.0    10.0  56.649998\n",
       "13      100.0    25.0  60.134274\n",
       "14      100.0    40.0  52.920101\n",
       "15      100.0    50.0  51.548753"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
