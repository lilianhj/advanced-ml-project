{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NNs with Law2Vec embeddings\n",
    "\n",
    "## Importing data, pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import product\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../data_pipeline/')\n",
    "import preprocessing as pre\n",
    "from training import TrainingModule\n",
    "\n",
    "SEED = 1312\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data(\n",
    "    'train_small.csv', 'val_small.csv', 'test_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "vectors = vocab.Vectors('../embeds/Law2Vec.100d.txt') # Law2Vec available from https://archive.org/details/Law2Vec\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=vectors,\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = torch.device('cuda' if USE_CUDA else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking pretrained vectors have been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors['medicare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['medicare']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbAvgPtEmbeds(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store training results\n",
    "\n",
    "df = pd.DataFrame(columns=['architecture', 'embeddings',\n",
    "                           'hidden', 'dropouts',\n",
    "                           'learning_rate', 'epochs',\n",
    "                           'dev_acc', 'dev_prec', 'dev_recall',\n",
    "                           'metric'])\n",
    "\n",
    "# Model architecture parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = TEXT.vocab.vectors.size(1)\n",
    "HIDDEN_SIZES = [10, 25, 40, 50]\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUTS = [0, 0.1, 0.25, 0.5, 0.75]\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Model training hyperparameters\n",
    "LEARNING_RATE = [0.01, 0.001, 0.0001]\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 10\n",
    "\n",
    "# Iterator over various model parameters\n",
    "param_iter = product (HIDDEN_SIZES, DROPOUTS, LEARNING_RATE)\n",
    "\n",
    "# Magic loop\n",
    "best_acc = (None, None)\n",
    "best_rec = (None, None)\n",
    "best_prec = (None, None)\n",
    "for i, (hidden_size, dropout, lr) in enumerate(param_iter):\n",
    "    print(f'Architecture #{i}\\n' + '-' * 20)\n",
    "    model = WordEmbAvgPtEmbeds(INPUT_DIM, EMBEDDING_SIZE, hidden_size,\n",
    "                OUTPUT_SIZE, PADDING_IDX, dropout_p=dropout)\n",
    "    \n",
    "    tm = TrainingModule(model, lr, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "    \n",
    "    best_models = tm.train_model(train_it, val_it)\n",
    "    \n",
    "    for metric, best_model in best_models.items():\n",
    "        row = [i, 'Law2Vec', hidden_size, dropout,\n",
    "               lr, EPOCHS, best_model.accuracy,\n",
    "               best_model.precision, best_model.recall, metric]\n",
    "        df.loc[len(df)] = row\n",
    "        if best_acc[0] is None or best_model.accuracy > best_acc[1]:\n",
    "            best_acc = (copy.deepcopy(best_model.model), best_model.accuracy)\n",
    "        if best_rec[0] is None or best_model.recall > best_rec[1]:\n",
    "            best_rec = (copy.deepcopy(best_model.model), best_model.recall)\n",
    "        if best_prec[0] is None or isnan(best_prec[1]) or\\\n",
    "          best_model.precision > best_prec[1]:\n",
    "            best_prec = (copy.deepcopy(best_model.model), best_model.precision)\n",
    "    \n",
    "    print('-' * 20 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PREFIX = '../results/SimpleNNLaw2Vec_'\n",
    "df.to_csv(f'{SAVE_PREFIX}models.csv')\n",
    "torch.save(best_acc[0], f'{SAVE_PREFIX}best_acc.pt')\n",
    "torch.save(best_rec[0], f'{SAVE_PREFIX}best_rec.pt')\n",
    "torch.save(best_prec[0], f'{SAVE_PREFIX}best_prec.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming embeddings have not been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicare_tensor = torch.LongTensor([TEXT.vocab.stoi['medicare']])\n",
    "if USE_CUDA:\n",
    "    medicare_tensor = medicare_tensor.cuda()\n",
    "model.embedding(medicare_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
