{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NNs with Law2Vec embeddings\n",
    "\n",
    "## Importing data, pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import product\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../data_pipeline/')\n",
    "import preprocessing as pre\n",
    "from training import TrainingModule\n",
    "\n",
    "SEED = 1312\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data(\n",
    "    'train_small.csv', 'val_small.csv', 'test_small.csv', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "vectors = vocab.Vectors('../embeds/Law2Vec.100d.txt') # Law2Vec available from https://archive.org/details/Law2Vec\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=vectors,\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = torch.device('cuda' if USE_CUDA else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking pretrained vectors have been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors['medicare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['medicare']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbAvgPtEmbeds(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store training results\n",
    "\n",
    "df = pd.DataFrame(columns=['architecture', 'embeddings',\n",
    "                           'hidden', 'dropouts',\n",
    "                           'learning_rate', 'epochs',\n",
    "                           'dev_acc', 'dev_prec', 'dev_recall',\n",
    "                           'metric'])\n",
    "\n",
    "# Model architecture parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = TEXT.vocab.vectors.size(1)\n",
    "HIDDEN_SIZES = [10, 25, 40, 50]\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUTS = [0, 0.1, 0.25, 0.5, 0.75]\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Model training hyperparameters\n",
    "LEARNING_RATE = [0.01, 0.001, 0.0001]\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 10\n",
    "\n",
    "# Iterator over various model parameters\n",
    "param_iter = product (HIDDEN_SIZES, DROPOUTS, LEARNING_RATE)\n",
    "\n",
    "# Magic loop\n",
    "best_acc = (None, None)\n",
    "best_rec = (None, None)\n",
    "best_prec = (None, None)\n",
    "for i, (hidden_size, dropout, lr) in enumerate(param_iter):\n",
    "    print(f'Architecture #{i}\\n' + '-' * 20)\n",
    "    model = WordEmbAvgPtEmbeds(INPUT_DIM, EMBEDDING_SIZE, hidden_size,\n",
    "                OUTPUT_SIZE, PADDING_IDX, dropout_p=dropout)\n",
    "    \n",
    "    tm = TrainingModule(model, lr, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "    \n",
    "    best_models = tm.train_model(train_it, val_it)\n",
    "    \n",
    "    for metric, best_model in best_models.items():\n",
    "        row = [i, embed_size, hidden_size, dropout,\n",
    "               lr, EPOCHS, best_model.accuracy,\n",
    "               best_model.precision, best_model.recall, metric]\n",
    "        df.loc[len(df)] = row\n",
    "        if best_acc[0] is None or isnan(best_acc[1]) or\\\n",
    "           best_model.accuracy > best_acc[1]:\n",
    "            best_acc = (copy.deepcopy(best_model.model), best_model.accuracy)\n",
    "        if best_rec[0] is None or isnan(best_rec[1]) or\\\n",
    "           best_model.recall > best_rec[1]:\n",
    "            best_rec = (copy.deepcopy(best_model.model), best_model.recall)\n",
    "        if best_prec[0] is None or isnan(best_prec[1]) or\\\n",
    "           best_model.precision > best_prec[1]:\n",
    "            best_prec = (copy.deepcopy(best_model.model), best_model.precision)\n",
    "    \n",
    "    print('-' * 20 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PREFIX = '../results/SimpleNNLaw2Vec_'\n",
    "df.to_csv(f'{SAVE_PREFIX}models.csv')\n",
    "torch.save(best_acc[0], f'{SAVE_PREFIX}best_acc.pt')\n",
    "torch.save(best_rec[0], f'{SAVE_PREFIX}best_rec.pt')\n",
    "torch.save(best_prec[0], f'{SAVE_PREFIX}best_prec.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming embeddings have not been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6415, -0.5367, -0.3537, -0.0634, -0.1798,  0.0626, -0.1836, -0.2705,\n",
       "          0.2504,  0.5061,  0.4746, -0.2351, -0.0465,  0.3184,  0.8974,  0.0470,\n",
       "         -0.2594,  0.3485, -0.3356,  0.1163,  0.2207,  0.2707,  0.4748,  0.1122,\n",
       "         -0.1188, -0.0790,  0.4377, -0.4711,  0.1401, -0.0234, -0.2009, -0.2143,\n",
       "          0.1335, -0.4407,  0.4077, -0.0634,  0.5104,  0.1820, -0.4729, -0.1758,\n",
       "          0.6194,  0.5708, -0.3034, -0.3658,  0.1609,  0.0753, -0.2024, -0.1472,\n",
       "          0.0665,  0.1823,  0.3091, -0.0913,  0.2495,  0.0777, -0.1873, -0.5850,\n",
       "         -0.3243,  0.1540, -0.5094,  0.6227,  0.1163, -0.6202, -0.4416, -0.3509,\n",
       "         -0.5760, -0.4837, -0.6283,  0.0938,  0.3528, -0.0674, -0.7097, -0.2053,\n",
       "         -0.6007, -0.1306,  0.0146, -0.0830,  0.5486, -0.2328, -0.3193,  0.1496,\n",
       "         -0.1635,  0.0755, -0.2594, -0.0317,  0.1249, -0.5599,  0.0722, -0.0369,\n",
       "          0.3139,  0.0102, -0.3353,  0.1142, -0.1163,  0.1505,  0.0952,  0.0206,\n",
       "         -0.0733, -0.4851,  0.4995,  0.0404]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medicare_tensor = torch.LongTensor([TEXT.vocab.stoi['medicare']])\n",
    "if USE_CUDA:\n",
    "    medicare_tensor = medicare_tensor.cuda()\n",
    "model.embedding(medicare_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
