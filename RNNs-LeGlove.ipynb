{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data, preparing batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/bf-repo/preprocessing.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  apply(lambda x: x[0: 1000000])\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data('train_small.csv', 'val_small.csv', 'test_small.csv', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vocab.Vectors('Law2Vec.100d.txt') # Law2Vec available from https://archive.org/details/Law2Vec\n",
    "TEXT.build_vocab(train_data, vectors=vectors, unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6415, -0.5367, -0.3537, -0.0634, -0.1798,  0.0626, -0.1836, -0.2705,\n",
       "         0.2504,  0.5061,  0.4746, -0.2351, -0.0465,  0.3184,  0.8974,  0.0470,\n",
       "        -0.2594,  0.3485, -0.3356,  0.1163,  0.2207,  0.2707,  0.4748,  0.1122,\n",
       "        -0.1188, -0.0790,  0.4377, -0.4711,  0.1401, -0.0234, -0.2009, -0.2143,\n",
       "         0.1335, -0.4407,  0.4077, -0.0634,  0.5104,  0.1820, -0.4729, -0.1758,\n",
       "         0.6194,  0.5708, -0.3034, -0.3658,  0.1609,  0.0753, -0.2024, -0.1472,\n",
       "         0.0665,  0.1823,  0.3091, -0.0913,  0.2495,  0.0777, -0.1873, -0.5850,\n",
       "        -0.3243,  0.1540, -0.5094,  0.6227,  0.1163, -0.6202, -0.4416, -0.3509,\n",
       "        -0.5760, -0.4837, -0.6283,  0.0938,  0.3528, -0.0674, -0.7097, -0.2053,\n",
       "        -0.6007, -0.1306,  0.0146, -0.0830,  0.5486, -0.2328, -0.3193,  0.1496,\n",
       "        -0.1635,  0.0755, -0.2594, -0.0317,  0.1249, -0.5599,  0.0722, -0.0369,\n",
       "         0.3139,  0.0102, -0.3353,  0.1142, -0.1163,  0.1505,  0.0952,  0.0206,\n",
       "        -0.0733, -0.4851,  0.4995,  0.0404])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors['medicare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6415, -0.5367, -0.3537, -0.0634, -0.1798,  0.0626, -0.1836, -0.2705,\n",
       "         0.2504,  0.5061,  0.4746, -0.2351, -0.0465,  0.3184,  0.8974,  0.0470,\n",
       "        -0.2594,  0.3485, -0.3356,  0.1163,  0.2207,  0.2707,  0.4748,  0.1122,\n",
       "        -0.1188, -0.0790,  0.4377, -0.4711,  0.1401, -0.0234, -0.2009, -0.2143,\n",
       "         0.1335, -0.4407,  0.4077, -0.0634,  0.5104,  0.1820, -0.4729, -0.1758,\n",
       "         0.6194,  0.5708, -0.3034, -0.3658,  0.1609,  0.0753, -0.2024, -0.1472,\n",
       "         0.0665,  0.1823,  0.3091, -0.0913,  0.2495,  0.0777, -0.1873, -0.5850,\n",
       "        -0.3243,  0.1540, -0.5094,  0.6227,  0.1163, -0.6202, -0.4416, -0.3509,\n",
       "        -0.5760, -0.4837, -0.6283,  0.0938,  0.3528, -0.0674, -0.7097, -0.2053,\n",
       "        -0.6007, -0.1306,  0.0146, -0.0830,  0.5486, -0.2328, -0.3193,  0.1496,\n",
       "        -0.1635,  0.0755, -0.2594, -0.0317,  0.1249, -0.5599,  0.0722, -0.0369,\n",
       "         0.3139,  0.0102, -0.3353,  0.1142, -0.1163,  0.1505,  0.0952,  0.0206,\n",
       "        -0.0733, -0.4851,  0.4995,  0.0404])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['medicare']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, input_size, embedding_size, hidden_size, output_size,\n",
    "                 num_layers, dropout, bidirectional, padding_idx):\n",
    "        super().__init__()\n",
    "        # self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors) # can abstract some\n",
    "        self.rnn = getattr(nn, rnn_type.upper())(embedding_size, hidden_size, num_layers,\n",
    "                                                 dropout=(droopout if num_layers > 1 else 0),\n",
    "                                                 bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(linear_inp, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        rnn_out = self.leakyrelu(rnn_out)\n",
    "        dropped_rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out\n",
    "    \n",
    "    def evaluate(self, preds, labels):\n",
    "        return self.loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y) # convert into float for division \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class Training_module():\n",
    "\n",
    "    def __init__(self, model, lr, pos_weight, use_cuda, epochs):\n",
    "        self.model = model\n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda:\n",
    "            model = model.cuda()\n",
    "            \n",
    "        self.epochs = epochs\n",
    "       \n",
    "        ##YOUR CODE HERE##\n",
    "        # Choose an optimizer. optim.Adam is a popular choice\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch in iterator:\n",
    "          # batch.text has the texts and batch.label has the labels.\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            ##YOUR CODE HERE##\n",
    "            text = batch.alj_text\n",
    "            target = batch.decision_binary\n",
    "            if self.use_cuda:\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            predictions = self.model.forward(text).squeeze()\n",
    "            loss = self.loss_fn(predictions, target)\n",
    "            accuracy = binary_accuracy(predictions, target)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs) or best_model is None:\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                #best_model.flatten_parameters() # would be good for RNNs\n",
    "            dev_accs.append(dev_acc[1])\n",
    "\n",
    "        return best_model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "\n",
    "                ##YOUR CODE HERE##\n",
    "                text = batch.alj_text\n",
    "                target = batch.decision_binary\n",
    "                if self.use_cuda:\n",
    "                    text = text.cuda()\n",
    "                    target = target.cuda()\n",
    "                predictions = self.model.forward(text).squeeze()\n",
    "                loss = self.loss_fn(predictions, target)\n",
    "                acc = binary_accuracy(predictions, target)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.2 Dev Loss:0.932163655757904\n",
      "Epoch 1: Dev Accuracy: 0.6 Dev Loss:0.9430160522460938\n",
      "Epoch 2: Dev Accuracy: 0.6 Dev Loss:0.9668230414390564\n",
      "Epoch 3: Dev Accuracy: 0.6 Dev Loss:0.9841375350952148\n",
      "Epoch 4: Dev Accuracy: 0.6 Dev Loss:1.0054779052734375\n",
      "Epoch 5: Dev Accuracy: 0.6 Dev Loss:1.0226577520370483\n",
      "Epoch 6: Dev Accuracy: 0.6 Dev Loss:1.0352592468261719\n",
      "Epoch 7: Dev Accuracy: 0.6 Dev Loss:1.0628145933151245\n",
      "Epoch 8: Dev Accuracy: 0.6 Dev Loss:1.0771770477294922\n",
      "Epoch 9: Dev Accuracy: 0.6 Dev Loss:1.098939061164856\n"
     ]
    }
   ],
   "source": [
    "# Model architecture parameters\n",
    "RNN_TYPES = ['RNN', 'LSTM']\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = vectors.vectors.size()[1]\n",
    "HIDDEN_SIZES = [1/3, 2/3]\n",
    "OUTPUT_SIZE = 1\n",
    "NUM_LAYERS = [1, 2]\n",
    "DROPOUTS = [0.5, 0.75]\n",
    "BIDIRECTIONALS = [False, True]\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Model training hyperparameters\n",
    "LEARNING_RATE = [0.01, 0.0001]\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 10\n",
    "\n",
    "# Iterator over various model parameters\n",
    "param_iter = product (RNN_TYPES, HIDDEN_SIZES, NUM_LAYERS, DROPOUTS,\n",
    "                      BIDIRECTIONALS, LEARNING_RATE)\n",
    "\n",
    "# Magic loop\n",
    "best_models = []\n",
    "for rnn_type, hidden_size, num_layers, dropout, bidirectional, lr in param_iter:\n",
    "    # Print out model type here\n",
    "    model = RNN(rnn_type, INPUT_DIM, EMBEDDING_SIZE, int(hidden_size * embed_size),\n",
    "                OUTPUT_SIZE, num_layers, dropout, bidirectional, PADDING_IDX)\n",
    "    tm = Training_module(model, lr, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "    best_model = tm.train_model(train_it, val_it)\n",
    "    best_models.append(best_model) # might be nice to save accuracy and recall numbers here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
