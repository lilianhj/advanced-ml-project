{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules and creating classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        \n",
    "class WordEmbAvgPtEmbeds(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx, two_layers=True, dropout_p=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        if two_layers == True:\n",
    "            self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim) \n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embedding_dim, output_dim)\n",
    "            self.linear2 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.mean(0)\n",
    "        if not self.linear2:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            output = self.relu(linear1_output)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        else:\n",
    "            linear1_output = self.linear1(embedded)\n",
    "            linear2_input = self.relu(linear1_output)\n",
    "            output = self.linear2(linear2_input)\n",
    "            output = self.drop_layer(output)\n",
    "            return output\n",
    "        \n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_type, input_size, embedding_size,\n",
    "                 hidden_size, output_size, num_layers, dropout,\n",
    "                 bidirectional, padding_idx):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "        \n",
    "        self.rnn = getattr(nn, rnn_type.upper())\\\n",
    "                          (embedding_size, hidden_size, num_layers,\n",
    "                           dropout=(dropout if num_layers > 1 else 0),\n",
    "                           bidirectional=bidirectional)\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(linear_inp, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        rnn_out = self.leakyrelu(rnn_out)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out\n",
    "    \n",
    "class RNNPtEmbeds(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_type, input_size, embedding_size,\n",
    "                 hidden_size, output_size, num_layers,\n",
    "                 dropout, bidirectional, padding_idx):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding\\\n",
    "                           .from_pretrained(TEXT.vocab.vectors)\n",
    "\n",
    "        self.rnn = getattr(nn, rnn_type.upper())\\\n",
    "                          (embedding_size, hidden_size, num_layers,\n",
    "                           dropout=(dropout if num_layers > 1 else 0),\n",
    "                           bidirectional=bidirectional)\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(linear_inp, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        rnn_out = self.leakyrelu(rnn_out)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "results_dir = '../results/'\n",
    "for file in os.listdir(results_dir):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.pt'):\n",
    "        model_path = os.path.join(results_dir, filename)\n",
    "        models[filename[:-3]] = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SimpleNNCustom_best_rec': WordEmbAvg(\n",
       "   (embedding): Embedding(32168, 32)\n",
       "   (linear1): Linear(in_features=32, out_features=10, bias=True)\n",
       "   (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0, inplace=False)\n",
       " ),\n",
       " 'SimpleNNCustom_best_prec': WordEmbAvg(\n",
       "   (embedding): Embedding(32168, 32)\n",
       "   (linear1): Linear(in_features=32, out_features=25, bias=True)\n",
       "   (linear2): Linear(in_features=25, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0.75, inplace=False)\n",
       " ),\n",
       " 'SimpleNNLaw2Vec_best_rec': WordEmbAvgPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (linear1): Linear(in_features=100, out_features=10, bias=True)\n",
       "   (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0, inplace=False)\n",
       " ),\n",
       " 'RNNCustom_best_acc': RNN(\n",
       "   (embedding): Embedding(10510, 32, padding_idx=1)\n",
       "   (rnn): RNN(32, 10, bidirectional=True)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " 'SimpleNNLaw2Vec_best_prec': WordEmbAvgPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (linear1): Linear(in_features=100, out_features=40, bias=True)\n",
       "   (linear2): Linear(in_features=40, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0, inplace=False)\n",
       " ),\n",
       " 'RNNLaw2Vec_best_rec': RNNPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (rnn): RNN(100, 33)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=33, out_features=1, bias=True)\n",
       " ),\n",
       " 'RNNCustom_best_rec': RNN(\n",
       "   (embedding): Embedding(10510, 32, padding_idx=1)\n",
       "   (rnn): RNN(32, 10)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       " ),\n",
       " 'SimpleNNCustom_best_acc': WordEmbAvg(\n",
       "   (embedding): Embedding(32168, 32)\n",
       "   (linear1): Linear(in_features=32, out_features=25, bias=True)\n",
       "   (linear2): Linear(in_features=25, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0.75, inplace=False)\n",
       " ),\n",
       " 'RNNCustom_best_prec': RNN(\n",
       "   (embedding): Embedding(10510, 32, padding_idx=1)\n",
       "   (rnn): RNN(32, 10, bidirectional=True)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " 'RNNLaw2Vec_best_acc': RNNPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (rnn): LSTM(100, 33, bidirectional=True)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=66, out_features=1, bias=True)\n",
       " ),\n",
       " 'RNNLaw2Vec_best_prec': RNNPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (rnn): LSTM(100, 33, bidirectional=True)\n",
       "   (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (linear): Linear(in_features=66, out_features=1, bias=True)\n",
       " ),\n",
       " 'SimpleNNLaw2Vec_best_acc': WordEmbAvgPtEmbeds(\n",
       "   (embedding): Embedding(32168, 100)\n",
       "   (linear1): Linear(in_features=100, out_features=10, bias=True)\n",
       "   (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (drop_layer): Dropout(p=0, inplace=False)\n",
       " )}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
