{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data, preparing batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "\n",
    "import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data('train_small.csv', 'val_small.csv', 'test_small.csv', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, input_size, embedding_size, hidden_size, output_size,\n",
    "                 num_layers, dropout, bidirectional, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.rnn = getattr(nn, rnn_type.upper())(embedding_size, hidden_size, num_layers,\n",
    "                                                 dropout=(dropout if num_layers > 1 else 0),\n",
    "                                                 bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(linear_inp, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        rnn_out = self.leakyrelu(rnn_out)\n",
    "        dropped_rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out\n",
    "    \n",
    "    def evaluate(self, preds, labels):\n",
    "        return self.loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y) # convert into float for division \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class Training_module():\n",
    "\n",
    "    def __init__(self, model, lr, pos_weight, use_cuda, epochs):\n",
    "        self.model = model\n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda:\n",
    "            model = model.cuda()\n",
    "            \n",
    "        self.epochs = epochs\n",
    "       \n",
    "        ##YOUR CODE HERE##\n",
    "        # Choose an optimizer. optim.Adam is a popular choice\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch in iterator:\n",
    "          # batch.text has the texts and batch.label has the labels.\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            ##YOUR CODE HERE##\n",
    "            text = batch.alj_text\n",
    "            target = batch.decision_binary\n",
    "            if self.use_cuda:\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            predictions = self.model.forward(text).squeeze()\n",
    "            loss = self.loss_fn(predictions, target)\n",
    "            accuracy = binary_accuracy(predictions, target)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs) or best_model is None:\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                #best_model.flatten_parameters() # would be good for RNNs\n",
    "            dev_accs.append(dev_acc[1])\n",
    "\n",
    "        return best_model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "\n",
    "                ##YOUR CODE HERE##\n",
    "                text = batch.alj_text\n",
    "                target = batch.decision_binary\n",
    "                if self.use_cuda:\n",
    "                    text = text.cuda()\n",
    "                    target = target.cuda()\n",
    "                predictions = self.model.forward(text).squeeze()\n",
    "                loss = self.loss_fn(predictions, target)\n",
    "                acc = binary_accuracy(predictions, target)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8291666666666666 Dev Loss:0.7731849054495493\n",
      "Epoch 1: Dev Accuracy: 0.8125 Dev Loss:0.7669222702582678\n",
      "Epoch 2: Dev Accuracy: 0.7416666666666667 Dev Loss:0.7978476484616598\n",
      "Epoch 3: Dev Accuracy: 0.1875 Dev Loss:0.827961598833402\n",
      "Epoch 4: Dev Accuracy: 0.725 Dev Loss:0.7987667322158813\n",
      "Epoch 5: Dev Accuracy: 0.7416666666666667 Dev Loss:0.8004347383975983\n",
      "Epoch 6: Dev Accuracy: 0.7416666666666667 Dev Loss:0.7941311647494634\n",
      "Epoch 7: Dev Accuracy: 0.1875 Dev Loss:0.797148734331131\n",
      "Epoch 8: Dev Accuracy: 0.27499999999999997 Dev Loss:0.792965774734815\n",
      "Epoch 9: Dev Accuracy: 0.1708333333333333 Dev Loss:0.8101414193709692\n",
      "Epoch 0: Dev Accuracy: 0.13749999999999998 Dev Loss:0.9032555222511292\n",
      "Epoch 1: Dev Accuracy: 0.13749999999999998 Dev Loss:0.8933917880058289\n",
      "Epoch 2: Dev Accuracy: 0.13749999999999998 Dev Loss:0.8841477980216345\n",
      "Epoch 3: Dev Accuracy: 0.13749999999999998 Dev Loss:0.879582424958547\n",
      "Epoch 4: Dev Accuracy: 0.13749999999999998 Dev Loss:0.8749605417251587\n",
      "Epoch 5: Dev Accuracy: 0.13749999999999998 Dev Loss:0.8722102691729864\n",
      "Epoch 6: Dev Accuracy: 0.13749999999999998 Dev Loss:0.8682004859050115\n"
     ]
    }
   ],
   "source": [
    "# Model architecture parameters\n",
    "RNN_TYPES = ['RNN', 'LSTM']\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZES = [32, 64, 128, 256]\n",
    "HIDDEN_SIZES = [1/3, 2/3]\n",
    "OUTPUT_SIZE = 1\n",
    "NUM_LAYERS = [1, 2]\n",
    "DROPOUTS = [0.5, 0.75]\n",
    "BIDIRECTIONALS = [False, True]\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Model training hyperparameters\n",
    "LEARNING_RATE = [0.01, 0.0001]\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 10\n",
    "\n",
    "# Iterator over various model parameters\n",
    "param_iter = product (RNN_TYPES, EMBEDDING_SIZES, HIDDEN_SIZES, NUM_LAYERS, DROPOUTS,\n",
    "                      BIDIRECTIONALS, LEARNING_RATE)\n",
    "\n",
    "# Magic loop\n",
    "best_models = []\n",
    "for rnn_type, embed_size, hidden_size, num_layers, dropout, bidirectional, lr in param_iter:\n",
    "    # Print out model type here\n",
    "    model = RNN(rnn_type, INPUT_DIM, embed_size, int(hidden_size * embed_size),\n",
    "                OUTPUT_SIZE, num_layers, dropout, bidirectional, PADDING_IDX)\n",
    "    tm = Training_module(model, lr, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "    best_model = tm.train_model(train_it, val_it)\n",
    "    best_models.append(best_model) # might be nice to save accuracy and recall numbers here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
