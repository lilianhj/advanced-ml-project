{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data, preparing batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pre\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/advanced-ml-project/preprocessing.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  apply(lambda x: x[0: 1000000])\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, val_data, TEXT, LABEL = pre.get_data('train_small.csv', 'val_small.csv', 'test_small.csv', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_it, test_it, val_it = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.alj_text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, input_size, embedding_size, hidden_size, output_size,\n",
    "                 num_layers, dropout, bidirectional, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.rnn = getattr(nn, rnn_type.upper())(embedding_size, hidden_size, num_layers, dropout=dropout,\n",
    "                                         bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        linear_inp = (hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "             \n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        rnn_out = rnn_out[-1]\n",
    "        dropped_rnn_out = self.dropout(rnn_out)\n",
    "        linear_out = self.linear(rnn_out)\n",
    "        return linear_out\n",
    "    \n",
    "    def evaluate(self, preds, labels):\n",
    "        return self.loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y) # convert into float for division \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class Training_module():\n",
    "\n",
    "    def __init__(self, model, lr, pos_weight, use_cuda, epochs):\n",
    "        self.model = model\n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda:\n",
    "            model = model.cuda()\n",
    "            \n",
    "        self.epochs = epochs\n",
    "       \n",
    "        ##YOUR CODE HERE##\n",
    "        # Choose an optimizer. optim.Adam is a popular choice\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch in iterator:\n",
    "          # batch.text has the texts and batch.label has the labels.\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            ##YOUR CODE HERE##\n",
    "            text = batch.alj_text\n",
    "            target = batch.decision_binary\n",
    "            if self.use_cuda:\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            predictions = self.model.forward(text).squeeze()\n",
    "            loss = self.loss_fn(predictions, target)\n",
    "            accuracy = binary_accuracy(predictions, target)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs) or best_model is None:\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                #best_model.flatten_parameters() # would be good for RNNs\n",
    "            dev_accs.append(dev_acc[1])\n",
    "\n",
    "        return best_model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "\n",
    "                ##YOUR CODE HERE##\n",
    "                text = batch.alj_text\n",
    "                target = batch.decision_binary\n",
    "                if self.use_cuda:\n",
    "                    text = text.cuda()\n",
    "                    target = target.cuda()\n",
    "                predictions = self.model.forward(text).squeeze()\n",
    "                loss = self.loss_fn(predictions, target)\n",
    "                acc = binary_accuracy(predictions, target)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8291666666666666 Dev Loss:0.7926198244094849\n",
      "Epoch 1: Dev Accuracy: 0.8125 Dev Loss:0.7885544002056122\n",
      "Epoch 2: Dev Accuracy: 0.7791666666666667 Dev Loss:0.791813482840856\n",
      "Epoch 3: Dev Accuracy: 0.7958333333333333 Dev Loss:0.7887606173753738\n",
      "Epoch 4: Dev Accuracy: 0.7458333333333332 Dev Loss:0.796973263223966\n",
      "Epoch 5: Dev Accuracy: 0.8624999999999999 Dev Loss:0.6752122516433398\n",
      "Epoch 6: Dev Accuracy: 0.8458333333333332 Dev Loss:0.7879130740960439\n",
      "Epoch 7: Dev Accuracy: 0.725 Dev Loss:0.7991801798343658\n",
      "Epoch 8: Dev Accuracy: 0.12083333333333333 Dev Loss:0.8146655907233556\n",
      "Epoch 9: Dev Accuracy: 0.6416666666666666 Dev Loss:1.01590450356404\n",
      "Epoch 10: Dev Accuracy: 0.6833333333333332 Dev Loss:0.7379422361652056\n",
      "Epoch 11: Dev Accuracy: 0.4583333333333333 Dev Loss:0.8417320549488068\n",
      "Epoch 12: Dev Accuracy: 0.525 Dev Loss:0.8037772178649902\n",
      "Epoch 13: Dev Accuracy: 0.5416666666666666 Dev Loss:0.8881389101346334\n",
      "Epoch 14: Dev Accuracy: 0.5416666666666666 Dev Loss:0.8596893002589544\n",
      "Epoch 15: Dev Accuracy: 0.6458333333333334 Dev Loss:0.8021454910437266\n",
      "Epoch 16: Dev Accuracy: 0.6458333333333334 Dev Loss:0.8518076489369074\n",
      "Epoch 17: Dev Accuracy: 0.7125 Dev Loss:0.8231223399440447\n",
      "Epoch 18: Dev Accuracy: 0.7083333333333334 Dev Loss:0.8535118947426478\n",
      "Epoch 19: Dev Accuracy: 0.6791666666666667 Dev Loss:1.0005681291222572\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = False\n",
    "PADDING_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = RNN('lstm', INPUT_DIM, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE,\n",
    "         NUM_LAYERS, DROPOUT, BIDIRECTIONAL, PADDING_IDX)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "train_len = 0\n",
    "train_pos = 0\n",
    "for batch in train_it:\n",
    "    train_len += len(batch.decision_binary)\n",
    "    train_pos += batch.decision_binary.sum().item()\n",
    "POS_WEIGHT = torch.tensor([(train_len - train_pos) / train_pos])\n",
    "if USE_CUDA:\n",
    "    POS_WEIGHT = POS_WEIGHT.cuda()\n",
    "EPOCHS = 20\n",
    "tm = Training_module(model, LEARNING_RATE, POS_WEIGHT, USE_CUDA, EPOCHS)\n",
    "\n",
    "#Training the model\n",
    "best_model = tm.train_model(train_it, val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "adv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
